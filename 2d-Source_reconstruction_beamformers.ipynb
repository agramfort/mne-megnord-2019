{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source reconstruction with beamformers in MNE-Python\n",
    "\n",
    "`\n",
    "Authors: Britta Westner <britta.wstnr@gmail.com>\n",
    "         Marijn van Vliet <w.m.vanvliet@gmail.com>\n",
    "         Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n",
    "`\n",
    "\n",
    "This notebook gives a quick introduction into source reconstruction with beamformers, featuring the _Linearly Constrained Minimum Variance beamformer_ (LCMV) and the _Dynamic Imaging of Coherent Sources_ (DICS).\n",
    "\n",
    "It will also highlight some of the latest beamforming-related implementations in MNE-Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is source reconstruction?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>EXPLANATION</b>:\n",
    "     <ul>\n",
    "    <li>Source reconstruction for MEG data <b>estimates the source activity</b> that produced the given sensor data. </li>\n",
    "     <li>For this, a <b> forward model</b> and an <b>inverse model</b> are needed.</li>\n",
    "         <li> The <b> foward model</b> describes how to go from <b> source to sensor space </b>. </li>\n",
    "         <li> The <b> inverse model </b> estimates the <b> source activity </b> - using the forward model.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<img src=\"sensor_to_source.png\" width=\"700\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is beamforming?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>EXPLANATION</b>:\n",
    "     <ul>\n",
    "         <li>Beamformers are one type of <b> inverse models</b>. </li>\n",
    "         <li> It uses a so-called <b> spatial filter </b> and estimates source activity <b> independently</b> on every source point.</li>\n",
    "         <li> It gives back a source space <b> activity pattern </b> and is often used on <b> volumetric </b> source spaces.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beamforming in MNE-Python\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>OVERVIEW</b>:\n",
    "     <ul>\n",
    "         <li>The beamformer module of MNE-Python got an almost complete makeover.</li>\n",
    "         <li>Started with Google Summer of Code 2017, (almost) finished with the MNE-Python Coding Sprint 2019. </li>\n",
    "         <li>Many new functions, new API, new parameters, new plotting, new tests, ...</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "# ... let's look at some of this fresh new stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "First, we'll load the data. We will use the ``sample`` dataset (visual and auditory stimulation) and only look at the trials with auditory input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: notebook. Using qt5 instead.\n"
     ]
    }
   ],
   "source": [
    "# for interactive figures:\n",
    "%matplotlib notebook  \n",
    "\n",
    "# imports\n",
    "import mne\n",
    "from mne.datasets import sample\n",
    "\n",
    "# paths to data\n",
    "data_path = sample.data_path()\n",
    "raw_fname = data_path + '/MEG/sample/sample_audvis_raw.fif'\n",
    "event_fname = data_path + '/MEG/sample/sample_audvis_raw-eve.fif'\n",
    "fwd_fname = data_path + '/MEG/sample/sample_audvis-meg-vol-7-fwd.fif'\n",
    "subjects_dir = data_path + '/subjects'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id, tmin, tmax = [1, 2], -0.2, 0.5\n",
    "\n",
    "# Setup for reading the raw data\n",
    "raw = mne.io.read_raw_fif(raw_fname, preload=True)\n",
    "events = mne.read_events(event_fname)\n",
    "\n",
    "# Set up pick list: EEG + MEG \n",
    "picks = mne.pick_types(raw.info, meg='mag', eeg=False, stim=True, eog=True,\n",
    "                       exclude='bads')\n",
    "\n",
    "# Pick the channels of interest\n",
    "raw.pick_channels([raw.ch_names[pick] for pick in picks])\n",
    "# Re-normalize our empty-room projectors, so they are fine after subselection\n",
    "raw.info.normalize_proj()\n",
    "\n",
    "# Epoching the data\n",
    "epochs = mne.Epochs(raw, events, event_id, tmin, tmax, \n",
    "                    baseline=(None, 0), preload=True, proj=True,\n",
    "                    reject=dict(mag=4e-12, eog=150e-6))\n",
    "\n",
    "# Compute the average\n",
    "evoked = epochs.average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at our sensor space data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked.plot_joint(ts_args=dict(time_unit='s'),\n",
    "                  topomap_args=dict(time_unit='s'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the forward model \n",
    "\n",
    "To save computation time, we will load the pre-computed forward model for our subject from disk. Note that this is not a _surface_ forward model, but a _volumetric_ forward model - with source points covering the whole volume of the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = mne.read_forward_solution(fwd_fname)\n",
    "forward = mne.convert_forward_solution(forward, surf_ori=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the data covariance matrix\n",
    "\n",
    "The computation of a beamformer spatial filter needs two ingredients: the forward model and a data covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cov = mne.compute_covariance(epochs, tmin=0.01, tmax=0.15, rank=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and apply the beamformer\n",
    "\n",
    "Now we have all the ingredients to **compute the spatial filter**! We will compute our beamformer with ``make_lcmv`` and then apply it to the evoked data computed above, by using ``apply_lcmv``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = mne.beamformer.make_lcmv(evoked.info, forward, data_cov, pick_ori='max-power')\n",
    "stc = mne.beamformer.apply_lcmv(evoked, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the source reconstruction\n",
    "\n",
    "Let's plot the output - the figure is **interactive**, so we can explore the estimated activity in source space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodin/Projects/scikit-learn/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "stc.plot(src=forward['src'], subject='sample', subjects_dir=subjects_dir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is new?\n",
    "\n",
    "### Functions and API\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>2-step API</b>: <br>\n",
    "    This makes it easy to use pre-computed filters, e.g., for statistical contrasts.\n",
    "     <ul>\n",
    "         <li> <tt> make_lcmv </tt> to compute the spatial filter </li>\n",
    "         <li> <tt> apply_lcmv </tt> for applying the spatial filter to any data </li>\n",
    "         <li> <tt> apply_lcmv </tt> exists for evoked data, epochs, raw data, and covariance matrixes (e.g., <tt> apply_lcmv_epochs, apply_lcmv_cov</tt>).</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "### Cool!\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Mixing channel types</b>: <br>\n",
    "    By supplying a <b> noise covariance matrix </b> it is possible to use several sensor types (e.g., MEG + EEG or mags + grads) for beamforming. <br>\n",
    "    The noise covariance matrix is used to <b> whiten the leadfield and data covariance matrix. </b>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Parameters and functionality (selection)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Supported is</b>: \n",
    "    <ul>\n",
    "        <li> <b> Leadfield normalization </b> using <tt> depth</tt>. </li>\n",
    "        <li> <b> Weight normalization </b> using <tt> weight_norm</tt>, this also enables estimation of the <b> Neural Activity Index</b>. </li>\n",
    "        <li> <b> Orientation selection </b>, e.g., normal to surface or maximal power, using <tt> pick_ori</tt>.\n",
    "</div>\n",
    "    \n",
    "    \n",
    "### Across toolbox stability\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Our output converges with FieldTrip's</b> ... <br>\n",
    "    ... and we constantly run tests (Continuous Integration) to ensure this.   \n",
    "</div>\n",
    "    \n",
    "<img src=\"fieldtrip_converge.png\" width=\"1000\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
